{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of a Full Recommender System through Contextual Decoupling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout this course, we have progressed with more and more advanced models for Recommender Systems. In this final step, we propose a complete Recommender System, that takes advantage of an large and vast dataset in order conduct an in depth investigation into the ideas that we have explored over the course of the last 7 weeks. Unlike the other projects that focused specifically on implement small recommender systems, our project here focuses on optimization of several ideas in order to implement a complete Recommender system, that takes time costs, and processing speed into consideration. Again our data is targeted torwards the movie genre (as again the dataset is readily available), but here we propose a more broad scope of techniques to minimize processing speed and optimize accuracy. As we move forward with technology, our amount of data continues to exponential grow, and the best way to process this vast source of data. Our target audience here would be any data host, with a continually growing data source, as we would like to be able to create a continually adaptable recommender system that does not require an epic amount of processing time. The way we propose to do this is to utilize the idea of Decoupling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Source and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project we will continue to use the MovieLens data source. However, in previous project we utilized that smaller of the dataset, as we are scaling this to a large source, we will move to the 1 million dataset. The dataset utilized in this project was slightly different, and required a significant amount of cleaning (for whatever reason they decided to save the data in a completely different format for the 1 million dataset). A complete copy of the code for data_extraction is given in the attached Data Extraction.py file in the github page. The datasets we extracted are given below: \n",
    "* Movie Content\n",
    "    * Total 3706 Movies Titles with 18 features\n",
    "    * Each column represents a genre:\"Action\",\"Adventure\",\"Animation\",\"Children's\",\"Comedy\",\"Crime\", \"Documentary\",\"Drama\",\"Fantasy\",\"Film-Noir\",\"Horror\",\"Musical\",\"Mystery\",\"Romance\",\"Sci-Fi\",\"Thriller\",\"War\",\"Western\"\n",
    "    * A specific movie can have more than 1 genre (the most any one movie has is 6)\n",
    "* User Content \n",
    "    * Total 6040 users with 29 features\n",
    "    * Features in order are Column 1 = Gender, Column 2-21 = specific job, and column 22-29 an age Range given as follows:\n",
    "        *  1:  \"Under 18\"\n",
    "        * 18:  \"18-24\"\n",
    "        * 25:  \"25-34\"\n",
    "        * 35:  \"35-44\"\n",
    "        * 45:  \"45-49\"\n",
    "        * 50:  \"50-55\"\n",
    "        * 56:  \"56+\"   \n",
    "The source code for data extraction is given below: (please note this is only the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "#Data file containing ratings data\n",
    "ratings = pd.read_csv(\"data/ratings.dat\", delimiter =\"::\", header=None, \n",
    "                      names=[\"user_id\", \"movies_id\",\"rating\",\"timestamp\"],engine='python')\n",
    "\n",
    "#Data containing movie data\n",
    "movies = pd.read_csv(\"data/movies.dat\", delimiter =\"::\", header=None, \n",
    "                      names=[\"movies_id\",\"title\",\"genre\"], engine='python')\n",
    "\n",
    "#Data containing User Data \n",
    "users = pd.read_csv(\"data/users.dat\", delimiter =\"::\", header=None, \n",
    "                      names=[\"user_id\", \"gender\",\"age\",\"occupation\",\"zip\"], engine='python')\n",
    "\n",
    "#Extracting Data Ratings data:\n",
    "ratings = ratings.drop([\"timestamp\"], 1)\n",
    "ratings = ratings.pivot(index = \"user_id\", columns = \"movies_id\", values = \"rating\")\n",
    "\n",
    "#Extracting Movies Data\n",
    "movie_titles = movies \n",
    "\n",
    "genres = [\"Action\",\"Adventure\",\"Animation\",\"Children's\",\"Comedy\",\"Crime\",\n",
    "          \"Documentary\",\"Drama\",\"Fantasy\",\"Film-Noir\",\"Horror\",\"Musical\",\n",
    "          \"Mystery\",\"Romance\",\"Sci-Fi\",\"Thriller\",\"War\",\"Western\"]\n",
    "\n",
    "movies = movies.drop([\"title\"],1)\n",
    "movies = movies.set_index(\"movies_id\")\n",
    "movies = movies['genre'].str.split(\"|\", expand = True)\n",
    "\n",
    "dummies1 = pd.get_dummies(movies[0], prefix='genre', prefix_sep='_')\n",
    "col_names_dummies = dummies1.columns.values\n",
    "\n",
    "dummies2 = pd.get_dummies(movies[1], prefix='genre', prefix_sep='_')\n",
    "col_names_dummies = dummies1.columns.values\n",
    "\n",
    "dummies3 = pd.get_dummies(movies[2], prefix='genre', prefix_sep='_')\n",
    "col_names_dummies = dummies1.columns.values\n",
    "\n",
    "dummies4 = pd.get_dummies(movies[3], prefix='genre', prefix_sep='_')\n",
    "col_names_dummies = dummies1.columns.values\n",
    "\n",
    "dummies5 = pd.get_dummies(movies[4], prefix='genre', prefix_sep='_')\n",
    "col_names_dummies = dummies1.columns.values\n",
    "\n",
    "dummies6 = pd.get_dummies(movies[5], prefix='genre', prefix_sep='_')\n",
    "col_names_dummies = dummies1.columns.values\n",
    "\n",
    "names = dummies1.columns.values\n",
    "\n",
    "dummies = dummies1.replace(0,np.nan)\n",
    "dummies2 = dummies2.replace(0,np.nan)\n",
    "dummies3 = dummies3.replace(0,np.nan)\n",
    "dummies4 = dummies4.replace(0,np.nan)\n",
    "dummies5 = dummies5.replace(0,np.nan)\n",
    "dummies = dummies.combine_first(dummies2)\n",
    "dummies = dummies.combine_first(dummies3)\n",
    "dummies = dummies.combine_first(dummies4)\n",
    "dummies = dummies.combine_first(dummies5)\n",
    "\n",
    "dummies = dummies.fillna(0)\n",
    "movies = dummies\n",
    "name = list(ratings.columns.values)\n",
    "movies = movies.T\n",
    "movies = movies[name]\n",
    "movies = movies.T\n",
    "\n",
    "#Extracting User Data\n",
    "users = users.set_index(\"user_id\")\n",
    "#Creating Dummy col for jobs\n",
    "dummies = pd.get_dummies(users['occupation'], prefix='job', prefix_sep='_')\n",
    "col_names_dummies = dummies.columns.values\n",
    "\n",
    "for i,value in enumerate(col_names_dummies):\n",
    "    users[value] = dummies.iloc[:,i]\n",
    "\n",
    "#Creating Dummy col for age\n",
    "dummies = pd.get_dummies(users['age'], prefix='age', prefix_sep='_')\n",
    "col_names_dummies = dummies.columns.values\n",
    "\n",
    "for i,value in enumerate(col_names_dummies):\n",
    "    users[value] = dummies.iloc[:,i]\n",
    "\n",
    "map_gender = {\"M\":0,\"F\":1}\n",
    "users = users.replace({\"gender\": map_gender})\n",
    "users = users.drop([\"age\",\"occupation\",\"zip\"], 1)\n",
    "\n",
    "# Borrowed some code here from Padebetttu et. al. -DATA643- Project 2\n",
    "ratings = ratings.apply(lambda x: x.fillna(x.mean(skipna = True)),axis=1)\n",
    "ratings = ratings.apply(lambda x: x-x.mean(), axis = 1).round(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like Project 4, the same Data Acquistion and Normalization step will be done with our final. The first two steps are a repeat of the methodology from Project 4's Proposal.\n",
    "* Data Acquistion and Normaliation\n",
    "    * The first step of this process will be to process the data. For the data will be organized into 3 matrices, a User-content matrix, an Item-Content Matrix and a User-Item Rating matrix. For the movie and User content matrices, each row will represent an movie or user, respectively, while the columns will represent a specific aspect of the movie or user. For istances, all the movie break downs will be either 0 or 1, indicating if it belongs to a specific genre (19 in total). For the users, each column will represnt a specific age range the belong to, their gender, and their occupation. \n",
    "* Matrix Factorization \n",
    "    * For our process here we will utilized standard Singular Value Decomposition \n",
    "    * The idea her would be to create multiple iterations, \"randomly\" splitting the dataset as a whole, completing the subset matrices, and then combining the subsets into the whole. After each iteration, we would then calculate the RMSE, and then repeat the process. Theoretically, after each step, the RMSE would decrease (if it is indeed the case)[5]. (I am attempting to obtain the publication for Barjasteh et. al [5]. At the moment I'd have to purchase it, but I signed up for free version through requests). From the video, there was some doubts on whether or not these results were accurate. Though they utilize a much more complex system, then the one I propose, if their stipulation that one can complete thest small subsets (that meet certain rank requirements) at random and produce accurate results. For our purposes here, due to the inability to get the AWS Sparks running, we are changing the \"Randomly\" selected datasets, and replacing them with specific contextual based subsets. First, each age range was extracted, with the understanding that people of different age groups will rate differently, based on personal preference. Using SVD, each of these individual subsets were then factored down and reproduced using a smaller feature set (k= 50 this number was chosen based on previous projects). Furthermore, we took several genre examples as well. I chose the 7 genres that I felt most distinctly represented the group as a whole: action, children's, comedy, drama, horror, romance, and sci-fi.One cause for concern however, was that each "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the above data, and using a modified SVD code, we created several subsets based on age and genre as described above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Context Break-down to \"Decouple\" Data for age\n",
    "Ratings_1= ratings.loc[list(users[users['age_1'] >0].T.columns.values)]\n",
    "Ratings_2= ratings.loc[list(users[users['age_18'] >0].T.columns.values)]\n",
    "Ratings_3= ratings.loc[list(users[users['age_25'] >0].T.columns.values)]\n",
    "Ratings_4= ratings.loc[list(users[users['age_35'] >0].T.columns.values)]\n",
    "Ratings_5= ratings.loc[list(users[users['age_45'] >0].T.columns.values)]\n",
    "Ratings_6= ratings.loc[list(users[users['age_50'] >0].T.columns.values)]\n",
    "Ratings_7= ratings.loc[list(users[users['age_56'] >0].T.columns.values)]\n",
    "\n",
    "#Context Break_down by Genre(NOT A COMPLETE LIST)\n",
    "genre_1= ratings[list(movies[movies['genre_Action'] >0].T.columns.values)]\n",
    "genre_2= ratings[list(movies[movies[\"genre_Children's\"] >0].T.columns.values)]\n",
    "genre_3= ratings[list(movies[movies['genre_Comedy'] >0].T.columns.values)]\n",
    "genre_4= ratings[list(movies[movies['genre_Drama'] >0].T.columns.values)]\n",
    "genre_5= ratings[list(movies[movies['genre_Horror'] >0].T.columns.values)]\n",
    "genre_6= ratings[list(movies[movies['genre_Romance'] >0].T.columns.values)]\n",
    "genre_7= ratings[list(movies[movies['genre_Sci-Fi'] >0].T.columns.values)]\n",
    "\n",
    "#SVD Function we are using (time efficient)\n",
    "def svd_red(movies,n):\n",
    "    col = list(movies.columns.values)\n",
    "    index = list(movies.index.values)\n",
    "    U1, s1, V1 = np.linalg.svd(movies, full_matrices=True)\n",
    "    k = np.zeros((len(s1),len(s1)),float)\n",
    "    np.fill_diagonal(k,s1)\n",
    "    k = k[:n,:n]\n",
    "    k = np.sqrt(k)\n",
    "    U2 = U1[:,:n]\n",
    "    V2 =V1[:,:n].T\n",
    "    Uk = np.dot(U2,k.T)\n",
    "    Vk = np.dot(k,V2)\n",
    "    R_red = np.dot(Uk,Vk)\n",
    "    R_red = pd.DataFrame(R_red, index = index, columns=col)\n",
    "    return R_red\n",
    "\n",
    "\n",
    "#Final Variable\n",
    "Total = svd_red(ratings, 50)\n",
    "#Ages\n",
    "rate_1 = svd_red(Ratings_1, 50)\n",
    "rate_2 = svd_red(Ratings_2, 50)\n",
    "rate_3 = svd_red(Ratings_3, 50)\n",
    "rate_4 = svd_red(Ratings_4, 50)\n",
    "rate_5 = svd_red(Ratings_5, 50)\n",
    "rate_6 = svd_red(Ratings_6, 50)\n",
    "rate_7 = svd_red(Ratings_7, 50)\n",
    "#Genres\n",
    "g_rate_1 = svd_red(genre_1, 50)\n",
    "g_rate_2 = svd_red(genre_2, 50)\n",
    "g_rate_3 = svd_red(genre_3, 50)\n",
    "g_rate_4 = svd_red(genre_4, 50)\n",
    "g_rate_5 = svd_red(genre_5, 50)\n",
    "g_rate_6 = svd_red(genre_6, 50)\n",
    "g_rate_7 = svd_red(genre_7, 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can take each level and recreate the initial dataframe from the individual pieces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Total_age = ratings.replace(0,np.nan)\n",
    "Total_age = Total_age.combine_first(rate_1)\n",
    "Total_age = Total_age.combine_first(rate_2)\n",
    "Total_age = Total_age.combine_first(rate_3)\n",
    "Total_age = Total_age.combine_first(rate_4)\n",
    "Total_age = Total_age.combine_first(rate_5)\n",
    "Total_age = Total_age.combine_first(rate_6)\n",
    "Total_age = Total_age.combine_first(rate_7)\n",
    "\n",
    "#Genre\n",
    "Total_genre = ratings.replace(0,np.nan)\n",
    "Total_genre = Total_genre.combine_first(g_rate_1)\n",
    "Total_genre = Total_genre.combine_first(g_rate_2)\n",
    "Total_genre = Total_genre.combine_first(g_rate_3)\n",
    "Total_genre = Total_genre.combine_first(g_rate_4)\n",
    "Total_genre = Total_genre.combine_first(g_rate_5)\n",
    "Total_genre = Total_genre.combine_first(g_rate_6)\n",
    "Total_genre = Total_genre.combine_first(g_rate_7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking these initialized datasets, we know have pre-fabricated dataframes that we can test against the straight SVD dataset. In theory the prefabricated datasets should create a more accurate picture, as they are broken down into similar contextual elements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Creates final Preduction based on Simmiliarity Matrix \n",
    "def pred_fin(Q,sim):\n",
    "    pred = Q.dot(sim) / np.array([np.abs(sim).sum(axis=1)]) \n",
    "    return pred\n",
    "\n",
    "#Using SVD on the Content Matrix to create similarity matrix\n",
    "movie_U = svd_red(movies,10)\n",
    "user_U = svd_red(users,10)\n",
    "\n",
    "item_sim = 1 - pairwise_distances(movie_U, metric='cosine')\n",
    "user_sim = 1 - pairwise_distances(user_U, metric='cosine')\n",
    "\n",
    "#Fill in NAN for the Genre\n",
    "Total_genre = Total_genre.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Genre_item_final = pred_fin(Total_genre.as_matrix().T,user_sim).T\n",
    "Genre_user_final = pred_fin(Total_genre.as_matrix(),item_sim)\n",
    "\n",
    "Age_item_final = pred_fin(Total_age.as_matrix().T,user_sim).T\n",
    "Age_user_final = pred_fin(Total_age.as_matrix(),item_sim)\n",
    "\n",
    "Total_item_final = pred_fin(Total.as_matrix().T,user_sim).T\n",
    "Total_user_final = pred_fin(Total.as_matrix(),item_sim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ratings_matrix = ratings.as_matrix()\n",
    "RMSE_user=[]\n",
    "RMSE_user.append(mean_squared_error(ratings_matrix[np.where(ratings_matrix != 0)], Genre_user_final[np.where(ratings_matrix != 0)]))\n",
    "RMSE_user.append(mean_squared_error(ratings_matrix[np.where(ratings_matrix != 0)], Age_user_final[np.where(ratings_matrix != 0)]))\n",
    "RMSE_user.append(mean_squared_error(ratings_matrix[np.where(ratings_matrix != 0)], Total_user_final[np.where(ratings_matrix != 0)]))\n",
    "\n",
    "RMSE_item=[]\n",
    "RMSE_item.append(mean_squared_error(ratings_matrix[np.where(ratings_matrix != 0)], Genre_item_final[np.where(ratings_matrix != 0)]))\n",
    "RMSE_item.append(mean_squared_error(ratings_matrix[np.where(ratings_matrix != 0)], Age_item_final[np.where(ratings_matrix != 0)]))\n",
    "RMSE_item.append(mean_squared_error(ratings_matrix[np.where(ratings_matrix != 0)], Total_item_final[np.where(ratings_matrix != 0)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RMSE_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RMSE_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "N = 3\n",
    "ind = np.arange(N)  # the x locations for the groups\n",
    "width = 0.35       # the width of the bars\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "rects1 = ax.bar(ind, RMSE_user, width, color='r')\n",
    "\n",
    "rects2 = ax.bar(ind + width, RMSE_item, width, color='g')\n",
    "\n",
    "# add some text for labels, title and axes ticks\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_title('RMSE of User-User vs Item-Item')\n",
    "ax.set_xticks(ind + width)\n",
    "ax.set_xticklabels(('Genre', 'Age', 'SVD'))\n",
    "\n",
    "ax.legend((rects1[0], rects2[0]), ('User-User', 'Item-Item'), loc = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Smaller Decoupled Datasets with ALS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ALS_Factor(Q, lambda_, n_factors, n_iterations):\n",
    "    W = Q != 0\n",
    "    W[W == True] = 1\n",
    "    W[W == False] = 0\n",
    "    # To be consistent with our Q matrix\n",
    "    W = W.astype(np.float64, copy=False)\n",
    "    m, n = Q.shape\n",
    "\n",
    "    X = 5 * np.random.rand(m, n_factors) \n",
    "    Y = 5 * np.random.rand(n_factors, n)\n",
    "\n",
    "\n",
    "    for ii in range(n_iterations):\n",
    "        for u, Wu in enumerate(W):\n",
    "            X[u] = np.linalg.solve(np.dot(Y, np.dot(np.diag(Wu), Y.T)) + lambda_ * np.eye(n_factors), np.dot(Y, np.dot(np.diag(Wu), Q[u].T))).T\n",
    "        for i, Wi in enumerate(W.T):\n",
    "            Y[:,i] = np.linalg.solve(np.dot(X.T, np.dot(np.diag(Wi), X)) + lambda_ * np.eye(n_factors), np.dot(X.T, np.dot(np.diag(Wi), Q[:, i])))\n",
    "\n",
    "        print('{}th iteration is completed'.format(ii))\n",
    "        \n",
    "    weighted_Q_hat = np.dot(X,Y)    \n",
    "    return weighted_Q_Hat, X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1]Nguyen, J., and Zhu, M.  Content-boosted matrix factorization techniques for recommender systems. *Statistical\n",
    "Analysis and Data Mining* 2013: 6(4):286–301. https://arxiv.org/pdf/1210.5631.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2]Vozalis, M. and Margaritis K.; Applying SVD on Generalized Item-based Filtering International Journal of Computer Science & Applications 2006; Vol.3 Is.3, pp 27- 51 http://www.tmrfindia.org/ijcsa/v3i34.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[3]Harper, F.M. and Konstan, J.; The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 2015: 5, 4, Article 19, 19 pages. http://dx.doi.org/10.1145/2827872 http://grouplens.org/datasets/movielens/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[4]G. Adomavicius, A. Tuzhilin, Context-Aware recommender Systems, in: F. Ricci, et al. (Ed.), Recommender Systems Handbook, 2011, pp. 217–253. http://ids.csom.umn.edu/faculty/gedas/nsfcareer/CARS-chapter-2010.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[5]Barjasteh, I., Forsati, R., et al. Cold-Start Item and User Recommendation with Decoupled Completion and Transduction\n",
    "https://www.cse.msu.edu/~forsati/cold-recsys2015.pdf (This is the slide, I am trying to obtain the PDF through the website)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

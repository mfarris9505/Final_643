{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of a Full Recommender System through the Process of Decoupling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout this course, we have progressed with more and more advanced models for Recommender Systems. In this final step, we propose a complete Recommender System, that takes advantage of an large and vast dataset in order conduct an in depth investigation into the ideas that we have explored over the course of the last 7 weeks. Unlike the other projects that focused specifically on implement small recommender systems, our project here focuses on optimization of several ideas in order to implement a complete Recommender system, that takes time costs, and processing speed into consideration. Again our data is targeted torwards the movie genre (as again the dataset is readily available), but here we propose a more broad scope of techniques to minimiza processing speed and optimize accuracy. As we move forward with technology, our amount of data continues to exponential grow, and the best way to process this vast source of data. Our target audience here would be any data host, with a continually growing data source, as we would like to be able to create a continually adaptable recommender system that does not require an epic amount of processing time. The way we propose to do this is to utilize the idea of Decoupling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Source and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project we will continue to use the MovieLens data source. However, in previous project we utilized that smaller of the dataset, as we are scaling this to a large source, we will move to the 10 million dataset (as this point I have run the code on Sparks, I will refrain from stating exactly howmuch of that dataset I will use, I am unsure of the cost for processing said data). As much of our source code will come from our previous experiments, in all liklihood that Data may change depending on our findings in the coming project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proposed Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like Project 4, the same Data Acquistion and Normalization step will be done with our final. The first two steps are a repeat of the methodology from Project 4's Proposal.\n",
    "* Data Acquistion and Normaliation\n",
    "    * The first step of this process will be to process the data. For the data will be organized into 3 matrices, a User-content matrix, an Item-Content Matrix and a User-Item Rating matrix. For the movie and User content matrices, each row will represent an movie or user, respectively, while the columns will represent a specific aspect of the movie or user. For istances, all the movie break downs will be either 0 or 1, indicating if it belongs to a specific genre (19 in total). For the users, each column will represnt a specific age range the belong to, their gender, and their occupation. Furthermore, the location will be included, however, it will be utilized only as a means of providing some contextual basis. \n",
    "    * Using the designated packages in python, normalization techniques will be applied to the Rating Matrix only.For this process, we are borrowing normalization techniques from Nguyen et. al [1]. The plan is to implement an ANOVA style normalization technique (time permitting) as opposed to a standard mean-centered approach. \n",
    "* Matrix Factorization \n",
    "    * Here is where the project defers. Though we plan to normalize the data using the above approach, we are not going to fill in the sparcity of the data. This time we plan on utilizing decoupling techniques to fill in the dataset. The goal here would be to use one of the techniques presented in Project 4 (SVD, ALS, etc.), to fill in this randomly selected subspace with predicted values. After that utilized a simple nearest neighbor analysis of this subspace (proposed for Project 4), where we use the highest nearest neighbor that also corresponds to the most similar movie type (or user type). \n",
    "    * The idea her would be to create multiple iterations, \"randomly\" splitting the dataset as a whole, completing the subset matrices, and then combining the subsets into the whole. After each iteration, we would then calculate the RMSE, and then repeat the process. Theoretically, after each step, the RMSE would decrease (if it is indeed the case)[5]. (I am attempting to obtain the publication for Barjasteh et. al [5]. At the moment I'd have to purchase it, but I signed up for free version through requests). From the video, there was some doubts on whether or not these results were accurate. Though they utilize a much more complex system, then the one I propose, if their stipulation that one can complete thest small subsets (that meet certain rank requirements) at random and produce accurate results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1]Nguyen, J., and Zhu, M.  Content-boosted matrix factorization techniques for recommender systems. *Statistical\n",
    "Analysis and Data Mining* 2013: 6(4):286–301. https://arxiv.org/pdf/1210.5631.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2]Vozalis, M. and Margaritis K.; Applying SVD on Generalized Item-based Filtering International Journal of Computer Science & Applications 2006; Vol.3 Is.3, pp 27- 51 http://www.tmrfindia.org/ijcsa/v3i34.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[3]Harper, F.M. and Konstan, J.; The MovieLens Datasets: History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 2015: 5, 4, Article 19, 19 pages. http://dx.doi.org/10.1145/2827872 http://grouplens.org/datasets/movielens/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[4]G. Adomavicius, A. Tuzhilin, Context-Aware recommender Systems, in: F. Ricci, et al. (Ed.), Recommender Systems Handbook, 2011, pp. 217–253. http://ids.csom.umn.edu/faculty/gedas/nsfcareer/CARS-chapter-2010.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[5]Barjasteh, I., Forsati, R., et al. Cold-Start Item and User Recommendation with Decoupled Completion and Transduction\n",
    "https://www.cse.msu.edu/~forsati/cold-recsys2015.pdf (This is the slide, I am trying to obtain the PDF through the website)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
